# LLM-Automation

## Overview
This project benchmarks the performance and consistency of large language models (LLMs) across a structured 9-step data analysis workflow. Using appointment data from the Government of New Brunswick (2013–2024), the benchmark investigates whether different LLMs—given the same analytical instructions—produce consistent, accurate, and reproducible outputs. The central research question is:

**Which government branch most frequently reappoints past appointees, and is this trend increasing or declining over time?**

We test three LLMs (GPT-4o, Claude Opus 4, Claude Sonnet 4) under multiple prompt protocols to evaluate their performance across accuracy, reproducibility, interpretive variation, and output variation.


## Running the Analysis

LLMs answer the question by generating scripts in numerical order, all scripts executed locally, and the intermediate results of each script are stored in folders like `scripts/someLLM/version#/execution#/analysis_data/`:

- `step1_combined_appointments.py`  
  Combines all 12 annual datasets and appends the year column.

- `step2_key_columns_data.py`  
  Extracts relevant columns: `name`, `position`, `org`, `year`, and `reappointed`.

- `step3_repeats_marked.py`  
  Marks `reappointed = True` for repeated `name`-`position`-`org` combinations.

- `step4_appointment_counts.py`  
  Counts total appointments per organization per year.

- `step5_reappointment_counts.py`  
  Counts reappointments per organization per year.

- `step6_reappointment_rates.py`  
  Calculates the reappointment rate as reappointments divided by total appointments.

- `step7_yearly_max_rates.py`  
  Identifies the organization with the highest reappointment rate per year and generates a time series plot.

- `step8_annual_proportions.py`  
  Calculates the overall reappointment proportion across government each year and visualizes the trend.

- `step9_regression_results.py`  
  Runs a linear regression on annual reappointment proportions to evaluate trend direction and significance.



## File Structure of this repository

* `LLM-Automation/`

  * `paper/` — Contains paper, reference, appendix
  * `precleaning/` — Raw data from IJF Appointments
  * `raw_data/` — Preprocessed 12 datasets for LLM to process

    * `appointments_2013.csv`
    * ...
    * `appointments_2024.csv`
  * `scripts/` — Code generated by 3 LLMs and human

    * `claudeopus4/`
    * `claudesonnet4/`
    * `gpt4o/` — Contains 3 versions

      * `version1/`
      * `version2/` — Contains 10 executions

        * `execution1/`
        * ...
        * `execution4/` — Contains Python scripts and intermediate results

          * `step1_combined_appointments.py`
          * ...
          * `analysis_data/`

            * `step1_combined_appointments.csv`
            * ...
            * `step9_regression_results.txt`
        * ...
        * `execution10/`
      * `version3/`
    * `human/` — Contains correct answer verified manually
  * `summary/` — Summary analysis over correctness of intermediate results
    * `claude_api/` - Ignore this for now
    * `general/` - Tests that run through all executions
    * `step_specific/` — Tests that are specific to each step
      * `step1/`
      * `step2/` 
        * `length` - Examine how many different types of results are there in terms of the number of rows they have
        * `correctness` - Examine whether the result is correct given its length
      * ... 
      * `step9/` 