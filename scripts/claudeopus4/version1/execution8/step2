#!/usr/bin/env python3
"""
Script to extract key columns from combined NB government appointments data.
Handles column name variations and creates filtered dataset.
"""

import pandas as pd
import numpy as np
from pathlib import Path
import sys
import re

def find_matching_column(df_columns, target_patterns):
    """
    Find column name that matches any of the target patterns.
    Case-insensitive matching with regex support.
    
    Args:
        df_columns: List of column names from DataFrame
        target_patterns: List of patterns to match against
        
    Returns:
        Matching column name or None
    """
    for col in df_columns:
        col_lower = col.lower().strip()
        for pattern in target_patterns:
            # Check exact match first
            if col_lower == pattern.lower():
                return col
            # Check if pattern is contained in column name
            if pattern.lower() in col_lower:
                return col
            # Check regex match
            if re.search(pattern.lower(), col_lower):
                return col
    return None

def main():
    # Define paths
    input_file = Path("scripts/claudeopus4/version1/execution8/analysis_data/step1_combined_appointments.csv")
    output_file = Path("scripts/claudeopus4/version1/execution8/analysis_data/step2_key_columns_data.csv")
    
    # Load the combined dataset
    try:
        print(f"Loading data from: {input_file}")
        df = pd.read_csv(input_file, encoding='utf-8')
        print(f"Successfully loaded dataset with shape: {df.shape}")
    except FileNotFoundError:
        print(f"Error: File {input_file} not found")
        sys.exit(1)
    except Exception as e:
        print(f"Error loading file: {e}")
        sys.exit(1)
    
    # Print all available columns
    print("\nAvailable columns in dataset:")
    for i, col in enumerate(df.columns):
        print(f"  {i+1}. {col}")
    
    # Define column mapping with variations
    column_mappings = {
        'reappointed': ['reappointed', 'reappointment', 're-appointed', 'is_reappointed'],
        'name': ['name', 'appointee_name', 'appointee', 'person_name', 'full_name'],
        'position': ['position', 'appointment', 'role', 'title', 'position_title', 'job_title'],
        'org': ['org', 'organization', 'organisation', 'agency', 'department', 'board', 'entity'],
        'year': ['year']  # Should already exist from step 1
    }
    
    # Find matching columns
    found_columns = {}
    missing_columns = []
    
    print("\nSearching for key columns...")
    for target_col, patterns in column_mappings.items():
        matched_col = find_matching_column(df.columns, patterns)
        if matched_col:
            found_columns[target_col] = matched_col
            print(f"  ✓ '{target_col}' mapped to: '{matched_col}'")
        else:
            missing_columns.append(target_col)
            print(f"  ✗ '{target_col}' not found")
    
    # Check if we have minimum required columns
    if 'year' not in found_columns:
        print("\nError: 'year' column is missing. This should have been added in step 1.")
        sys.exit(1)
    
    if len(found_columns) < 3:  # At least year + 2 other columns
        print(f"\nError: Too few columns found ({len(found_columns)}). Need at least 3 columns.")
        print("Please check column names in the dataset.")
        sys.exit(1)
    
    # Create new dataframe with found columns
    selected_columns = list(found_columns.values())
    filtered_df = df[selected_columns].copy()
    
    # Rename columns to standardized names
    rename_mapping = {v: k for k, v in found_columns.items()}
    filtered_df.rename(columns=rename_mapping, inplace=True)
    
    print(f"\nCreated filtered dataset with shape: {filtered_df.shape}")
    print(f"Columns included: {list(filtered_df.columns)}")
    
    # Analyze missing values
    print("\nMissing values analysis:")
    print("-" * 50)
    total_rows = len(filtered_df)
    
    for col in filtered_df.columns:
        null_count = filtered_df[col].isnull().sum()
        null_pct = (null_count / total_rows) * 100
        print(f"{col:15} | Missing: {null_count:6} ({null_pct:5.2f}%)")
    
    # Additional analysis for reappointed column if present
    if 'reappointed' in filtered_df.columns:
        print("\nReappointed column analysis:")
        print("-" * 30)
        
        # Check data type and unique values
        print(f"Data type: {filtered_df['reappointed'].dtype}")
        unique_vals = filtered_df['reappointed'].unique()
        print(f"Unique values: {unique_vals}")
        
        # Value counts
        print("\nValue distribution:")
        value_counts = filtered_df['reappointed'].value_counts(dropna=False)
        for val, count in value_counts.items():
            pct = (count / total_rows) * 100
            print(f"  {val}: {count} ({pct:.2f}%)")
    
    # Save the filtered dataset
    try:
        filtered_df.to_csv(output_file, index=False, encoding='utf-8')
        print(f"\nFiltered dataset saved to: {output_file}")
    except Exception as e:
        print(f"Error saving file: {e}")
        sys.exit(1)
    
    # Print summary statistics
    print("\nSummary Statistics:")
    print("=" * 50)
    print(f"Total rows: {len(filtered_df)}")
    print(f"Total columns: {len(filtered_df.columns)}")
    
    if 'year' in filtered_df.columns:
        print(f"\nYear range: {filtered_df['year'].min()} - {filtered_df['year'].max()}")
        print(f"Unique years: {sorted(filtered_df['year'].unique())}")
    
    if 'org' in filtered_df.columns:
        print(f"\nUnique organizations: {filtered_df['org'].nunique()}")
        print("Top 5 organizations by appointment count:")
        top_orgs = filtered_df['org'].value_counts().head()
        for org, count in top_orgs.items():
            print(f"  {org}: {count}")
    
    # Show sample of the data
    print("\nSample of filtered data (first 5 rows):")
    print(filtered_df.head())
    
    print("\nScript completed successfully!")

if __name__ == "__main__":
    main()