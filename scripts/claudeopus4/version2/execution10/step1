#!/usr/bin/env python3
"""
Step 1: Combine the 12 raw datasets
This script combines appointment data from 2013-2024 into a single CSV file.
"""

import pandas as pd
import numpy as np
from pathlib import Path
import sys

def main():
    """Main function to combine all appointment datasets."""
    
    # Define paths
    raw_data_dir = Path("raw_data")
    output_dir = Path("scripts/claudeopus4/version2/execution10/analysis_data")
    
    # Create output directory if it doesn't exist
    output_dir.mkdir(parents=True, exist_ok=True)
    print(f"Output directory created/verified: {output_dir}")
    
    # Initialize list to store all dataframes
    all_dfs = []
    
    # Years to process
    years = range(2013, 2025)  # 2013 to 2024 inclusive
    
    # Track statistics
    files_found = 0
    files_missing = []
    total_rows = 0
    
    print("\n" + "="*60)
    print("STEP 1: COMBINING RAW DATASETS")
    print("="*60 + "\n")
    
    # Process each year
    for year in years:
        filename = f"appointments_{year}.csv"
        filepath = raw_data_dir / filename
        
        try:
            # Check if file exists
            if not filepath.exists():
                print(f"⚠️  Warning: {filename} not found at {filepath}")
                files_missing.append(year)
                continue
            
            # Read the CSV file
            print(f"Processing {filename}...")
            df = pd.read_csv(filepath, encoding='utf-8')
            
            # Add year column for tracking
            df['source_year'] = year
            
            # Print basic info about this file
            print(f"  ✓ Loaded: {len(df)} rows, {len(df.columns)} columns")
            
            # Track column names for validation
            if files_found == 0:
                first_columns = set(df.columns)
                print(f"  ✓ Columns: {', '.join(df.columns)}")
            else:
                # Check for column consistency
                current_columns = set(df.columns)
                missing_cols = first_columns - current_columns
                extra_cols = current_columns - first_columns
                
                if missing_cols:
                    print(f"  ⚠️  Missing columns compared to first file: {missing_cols}")
                if extra_cols:
                    print(f"  ⚠️  Extra columns compared to first file: {extra_cols}")
            
            # Add to our list
            all_dfs.append(df)
            files_found += 1
            total_rows += len(df)
            
        except Exception as e:
            print(f"❌ Error processing {filename}: {str(e)}")
            files_missing.append(year)
            continue
    
    print("\n" + "-"*60)
    
    # Check if we have any data to combine
    if not all_dfs:
        print("❌ ERROR: No data files found! Cannot continue.")
        sys.exit(1)
    
    # Combine all dataframes
    print(f"\nCombining {files_found} datasets...")
    combined_df = pd.concat(all_dfs, ignore_index=True, sort=False)
    
    # Validate combined dataset
    print("\nValidating combined dataset...")
    print(f"  ✓ Total rows: {len(combined_df)}")
    print(f"  ✓ Expected rows: {total_rows}")
    print(f"  ✓ Match: {'Yes' if len(combined_df) == total_rows else 'No'}")
    print(f"  ✓ Total columns: {len(combined_df.columns)}")
    
    # Check for critical columns
    critical_columns = ['name', 'position', 'reappointed']
    missing_critical = []
    for col in critical_columns:
        if col not in combined_df.columns:
            missing_critical.append(col)
    
    if missing_critical:
        print(f"  ⚠️  Warning: Missing critical columns: {missing_critical}")
    else:
        print(f"  ✓ All critical columns present")
    
    # Analyze data quality
    print("\nData quality analysis:")
    print(f"  ✓ Duplicate rows: {combined_df.duplicated().sum()}")
    
    # Check missing values for key columns
    for col in combined_df.columns:
        if col in critical_columns or col in ['org', 'organization']:
            missing_count = combined_df[col].isna().sum()
            missing_pct = (missing_count / len(combined_df)) * 100
            print(f"  ✓ Missing values in '{col}': {missing_count} ({missing_pct:.1f}%)")
    
    # Check reappointed column values
    if 'reappointed' in combined_df.columns:
        print(f"\n  Reappointed column analysis:")
        print(f"    - Data type: {combined_df['reappointed'].dtype}")
        print(f"    - Unique values: {combined_df['reappointed'].unique()}")
        print(f"    - Value counts:")
        value_counts = combined_df['reappointed'].value_counts(dropna=False)
        for val, count in value_counts.items():
            print(f"      {val}: {count} ({(count/len(combined_df)*100):.1f}%)")
    
    # Year distribution
    print("\nYear distribution:")
    year_counts = combined_df['source_year'].value_counts().sort_index()
    for year, count in year_counts.items():
        print(f"  ✓ {year}: {count} appointments")
    
    # Save combined dataset
    output_file = output_dir / "step1_combined_appointments.csv"
    print(f"\nSaving combined dataset to: {output_file}")
    combined_df.to_csv(output_file, index=False, encoding='utf-8')
    print(f"✓ Successfully saved {len(combined_df)} rows")
    
    # Summary report
    print("\n" + "="*60)
    print("SUMMARY")
    print("="*60)
    print(f"Files processed: {files_found}/{len(years)}")
    if files_missing:
        print(f"Missing years: {files_missing}")
    print(f"Total appointments: {len(combined_df)}")
    print(f"Date range: {combined_df['source_year'].min()} - {combined_df['source_year'].max()}")
    print(f"Output saved to: {output_file}")
    print("\n✓ Step 1 completed successfully!")
    
    return combined_df

if __name__ == "__main__":
    # Run the main function
    try:
        combined_data = main()
    except Exception as e:
        print(f"\n❌ Fatal error: {str(e)}")
        sys.exit(1)