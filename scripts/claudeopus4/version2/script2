"""
Step 2: Extract Key Columns
This script extracts and keeps only the key columns: reappointed, name, position, org
"""

import pandas as pd
import numpy as np
from pathlib import Path
import sys

def extract_key_columns():
    """
    Extracts key columns (reappointed, name, position, org) from the combined dataset.
    """
    # Define paths
    input_path = Path('scripts/claudeopus4/version2/analysis_data')
    input_file = input_path / 'step1_combined_appointments.csv'
    output_file = input_path / 'step2_key_columns_data.csv'
    
    # Check if input file exists
    if not input_file.exists():
        print(f"ERROR: Input file {input_file} does not exist!")
        print("Please run Step 1 first to create the combined dataset.")
        sys.exit(1)
    
    print(f"Reading combined dataset from: {input_file}")
    
    try:
        # Read the combined dataset
        df = pd.read_csv(input_file, encoding='utf-8')
        print(f"Dataset loaded successfully.")
        print(f"  - Total rows: {len(df)}")
        print(f"  - Total columns: {len(df.columns)}")
        
        # Define required columns
        required_columns = ['reappointed', 'name', 'position', 'org']
        
        # Check which required columns exist
        print(f"\nChecking for required columns...")
        existing_columns = []
        missing_columns = []
        
        for col in required_columns:
            if col in df.columns:
                existing_columns.append(col)
                print(f"  ✓ '{col}' found")
            else:
                missing_columns.append(col)
                print(f"  ✗ '{col}' NOT FOUND")
        
        # If any required columns are missing, try to find similar columns
        if missing_columns:
            print(f"\nSearching for similar column names...")
            all_columns = df.columns.tolist()
            
            for missing_col in missing_columns:
                similar = [col for col in all_columns if missing_col in col.lower()]
                if similar:
                    print(f"  - Similar to '{missing_col}': {similar}")
        
        # Abort if critical columns are missing
        if not existing_columns:
            print("\nERROR: No required columns found in the dataset!")
            sys.exit(1)
        
        # Extract only the existing key columns
        df_key = df[existing_columns].copy()
        
        print(f"\nExtracted columns: {', '.join(existing_columns)}")
        print(f"Dataset shape after extraction: {df_key.shape}")
        
        # Analyze each column
        print(f"\nColumn analysis:")
        
        for col in existing_columns:
            print(f"\n{col.upper()} column:")
            print(f"  - Data type: {df_key[col].dtype}")
            print(f"  - Non-null values: {df_key[col].notna().sum()} ({df_key[col].notna().sum()/len(df_key)*100:.2f}%)")
            print(f"  - Null values: {df_key[col].isna().sum()} ({df_key[col].isna().sum()/len(df_key)*100:.2f}%)")
            print(f"  - Unique values: {df_key[col].nunique()}")
            
            # Special analysis for reappointed column
            if col == 'reappointed':
                print(f"  - Value distribution:")
                value_counts = df_key[col].value_counts(dropna=False)
                for val, count in value_counts.items():
                    print(f"    - {val}: {count} ({count/len(df_key)*100:.2f}%)")
                
                # Check data type and convert if necessary
                if df_key[col].dtype == 'object':
                    print(f"\n  Converting 'reappointed' column to boolean...")
                    # Common true/false variations
                    true_values = ['true', 'True', 'TRUE', 'yes', 'Yes', 'YES', '1', 1, True]
                    false_values = ['false', 'False', 'FALSE', 'no', 'No', 'NO', '0', 0, False]
                    
                    # Create a mapping
                    df_key['reappointed_original'] = df_key[col]
                    df_key['reappointed_bool'] = df_key[col].apply(
                        lambda x: True if x in true_values else (False if x in false_values else np.nan)
                    )
                    
                    # Check conversion results
                    conversion_stats = pd.crosstab(
                        df_key['reappointed_original'], 
                        df_key['reappointed_bool'], 
                        margins=True, 
                        dropna=False
                    )
                    print(f"\n  Conversion results:")
                    print(conversion_stats)
                    
                    # Replace original column with boolean version
                    df_key['reappointed'] = df_key['reappointed_bool']
                    df_key.drop(['reappointed_original', 'reappointed_bool'], axis=1, inplace=True)
            
            # Sample values for other columns
            elif col in ['name', 'position', 'org']:
                print(f"  - Sample values (first 5 unique):")
                unique_vals = df_key[col].dropna().unique()[:5]
                for val in unique_vals:
                    print(f"    - {val}")
        
        # Check for rows with all key data
        print(f"\nData completeness analysis:")
        complete_rows = df_key.notna().all(axis=1).sum()
        print(f"  - Rows with all key columns filled: {complete_rows} ({complete_rows/len(df_key)*100:.2f}%)")
        
        # Analyze missing patterns
        print(f"\nMissing data patterns:")
        missing_patterns = df_key.isna().value_counts()
        print(f"  - Different missing patterns: {len(missing_patterns)}")
        for pattern, count in missing_patterns.head(5).items():
            if isinstance(pattern, tuple):
                missing_cols = [existing_columns[i] for i, is_missing in enumerate(pattern) if is_missing]
                if missing_cols:
                    print(f"    - Missing {missing_cols}: {count} rows")
                else:
                    print(f"    - No missing data: {count} rows")
            else:
                # Single column case
                if pattern:
                    print(f"    - Missing data: {count} rows")
                else:
                    print(f"    - No missing data: {count} rows")
        
        # Summary statistics
        print(f"\n{'='*50}")
        print(f"SUMMARY:")
        print(f"  - Input rows: {len(df)}")
        print(f"  - Output rows: {len(df_key)}")
        print(f"  - Columns extracted: {len(existing_columns)} of {len(required_columns)} requested")
        print(f"  - Missing columns: {missing_columns if missing_columns else 'None'}")
        
        # Save the extracted data
        df_key.to_csv(output_file, index=False, encoding='utf-8')
        print(f"\nKey columns data saved to: {output_file}")
        
        # Return summary information
        return {
            'total_rows': len(df_key),
            'columns_extracted': existing_columns,
            'missing_columns': missing_columns,
            'complete_rows': complete_rows,
            'output_file': str(output_file)
        }
        
    except Exception as e:
        print(f"\nERROR: Failed to process the dataset: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    # Run the extraction process
    summary = extract_key_columns()
    print("\nStep 2 completed successfully!")