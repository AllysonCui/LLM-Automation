#!/usr/bin/env python3

"""
New Brunswick Government Appointments Data Combiner

This script loads appointment data from 2013-2024 CSV files, adds year columns,
combines them into a single dataset, and saves the result.

Data Structure Assumptions:
- All CSV files have consistent column structure (16 columns as per sample)
- Files are named appointments_YYYY.csv where YYYY is the year 2013-2024
- Files are located in raw_data/ directory relative to script location
- CSV files use UTF-8 encoding with standard comma delimiters

Author: Claude Sonnet 4
Date: June 30, 2025
"""

import pandas as pd
import numpy as np
import os
from pathlib import Path
import sys


def load_and_combine_appointments():
    """
    Load all appointment CSV files (2013-2024), add year columns, and combine.
    
    Returns:
        pd.DataFrame: Combined dataset with all appointments and year column
    """
    
    # Define the years to process
    years = range(2013, 2025)  # 2013 through 2024
    
    # List to store individual DataFrames
    dataframes = []
    
    # Expected columns based on the sample data
    expected_columns = [
        'name', 'position', 'org', 'location', 'region', 'posted_date',
        'start_date', 'end_date', 'term_length', 'acts', 'remuneration',
        'reappointed', 'oic', 'href', 'body', 'link'
    ]
    
    print("Loading appointment data files...")
    print("-" * 50)
    
    for year in years:
        filename = f"raw_data/appointments_{year}.csv"
        
        try:
            # Check if file exists
            if not os.path.exists(filename):
                print(f"WARNING: File {filename} not found - skipping")
                continue
            
            # Load the CSV file
            df = pd.read_csv(filename, encoding='utf-8')
            
            # Validate column structure
            if len(df.columns) != len(expected_columns):
                print(f"WARNING: {filename} has {len(df.columns)} columns, expected {len(expected_columns)}")
            
            # Add year column
            df['year'] = year
            
            # Append to list
            dataframes.append(df)
            
            print(f"✓ Loaded {filename}: {len(df)} rows, {len(df.columns)} columns")
            
        except Exception as e:
            print(f"ERROR loading {filename}: {str(e)}")
            continue
    
    # Check if any data was loaded
    if not dataframes:
        raise ValueError("No data files were successfully loaded!")
    
    print(f"\nCombining {len(dataframes)} datasets...")
    
    # Combine all DataFrames
    combined_df = pd.concat(dataframes, ignore_index=True, sort=False)
    
    print(f"✓ Successfully combined datasets")
    
    return combined_df


def save_combined_data(df, output_dir="scripts/claudesonnet4/version1/execution6/analysis_data"):
    """
    Save the combined dataset to CSV file.
    
    Args:
        df (pd.DataFrame): Combined appointments dataset
        output_dir (str): Directory to save the output file
    """
    
    # Create output directory if it doesn't exist
    Path(output_dir).mkdir(parents=True, exist_ok=True)
    
    # Define output file path
    output_file = os.path.join(output_dir, "step1_combined_appointments.csv")
    
    try:
        # Save to CSV
        df.to_csv(output_file, index=False, encoding='utf-8')
        print(f"✓ Saved combined dataset to: {output_file}")
        
    except Exception as e:
        print(f"ERROR saving file: {str(e)}")
        raise


def print_dataset_info(df):
    """
    Print basic information about the combined dataset.
    
    Args:
        df (pd.DataFrame): Combined appointments dataset
    """
    
    print("\n" + "=" * 60)
    print("COMBINED DATASET SUMMARY")
    print("=" * 60)
    
    # Basic shape information
    print(f"Dataset shape: {df.shape[0]:,} rows × {df.shape[1]} columns")
    
    # Year distribution
    if 'year' in df.columns:
        year_counts = df['year'].value_counts().sort_index()
        print(f"\nRecords by year:")
        for year, count in year_counts.items():
            print(f"  {year}: {count:,} appointments")
    
    # Column information
    print(f"\nColumns: {list(df.columns)}")
    
    # Data types
    print(f"\nData types:")
    for col, dtype in df.dtypes.items():
        print(f"  {col}: {dtype}")
    
    # Memory usage
    memory_mb = df.memory_usage(deep=True).sum() / 1024 / 1024
    print(f"\nMemory usage: {memory_mb:.2f} MB")
    
    # Missing values summary
    missing_counts = df.isnull().sum()
    if missing_counts.sum() > 0:
        print(f"\nMissing values:")
        for col, count in missing_counts.items():
            if count > 0:
                pct = (count / len(df)) * 100
                print(f"  {col}: {count:,} ({pct:.1f}%)")
    else:
        print(f"\nNo missing values found")


def main():
    """Main execution function."""
    
    print("New Brunswick Government Appointments Data Combiner")
    print("=" * 60)
    
    try:
        # Load and combine all appointment data
        combined_df = load_and_combine_appointments()
        
        # Save the combined dataset
        save_combined_data(combined_df)
        
        # Print summary information
        print_dataset_info(combined_df)
        
        print(f"\n✓ Process completed successfully!")
        
    except Exception as e:
        print(f"\n❌ Process failed: {str(e)}")
        sys.exit(1)


if __name__ == "__main__":
    main()