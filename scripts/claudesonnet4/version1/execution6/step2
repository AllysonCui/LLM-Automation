#!/usr/bin/env python3

"""
Key Columns Extractor for New Brunswick Government Appointments

This script loads the combined appointments dataset and extracts key columns:
- reappointed (boolean indicator)
- name (appointee name)
- position (role/title)
- org (organization/department)
- year (appointment year)

The script handles variations in column naming and creates a clean dataset
with only the essential columns for analysis.

Author: Claude Sonnet 4
Date: June 30, 2025
"""

import pandas as pd
import numpy as np
import os
from pathlib import Path
import sys
import re


def find_column_match(df_columns, target_patterns):
    """
    Find the best matching column name from a list of patterns.
    
    Args:
        df_columns (list): List of actual column names in the DataFrame
        target_patterns (list): List of possible column name patterns to match
    
    Returns:
        str or None: The matching column name, or None if no match found
    """
    
    # Convert all columns to lowercase for case-insensitive matching
    columns_lower = [col.lower().strip() for col in df_columns]
    
    for pattern in target_patterns:
        pattern_lower = pattern.lower().strip()
        
        # Exact match first
        if pattern_lower in columns_lower:
            original_index = columns_lower.index(pattern_lower)
            return df_columns[original_index]
        
        # Partial match (contains the pattern)
        for i, col in enumerate(columns_lower):
            if pattern_lower in col or col in pattern_lower:
                return df_columns[i]
    
    return None


def identify_key_columns(df):
    """
    Identify key columns in the dataset using flexible matching.
    
    Args:
        df (pd.DataFrame): Input dataset
    
    Returns:
        dict: Mapping of standard names to actual column names
    """
    
    # Define possible column name variations for each key column
    column_patterns = {
        'reappointed': ['reappointed', 'reappoint', 're-appointed', 'renewed', 'renewal'],
        'name': ['name', 'appointee', 'person', 'individual', 'full_name', 'fullname'],
        'position': ['position', 'title', 'role', 'appointment', 'job_title', 'post'],
        'org': ['org', 'organization', 'organisation', 'department', 'agency', 'ministry', 'body'],
        'year': ['year', 'appointment_year', 'appt_year', 'yr']
    }
    
    column_mapping = {}
    
    print("Identifying key columns...")
    print("-" * 40)
    
    for standard_name, patterns in column_patterns.items():
        matched_column = find_column_match(df.columns.tolist(), patterns)
        
        if matched_column:
            column_mapping[standard_name] = matched_column
            print(f"✓ {standard_name.upper():12} -> '{matched_column}'")
        else:
            print(f"❌ {standard_name.upper():12} -> NOT FOUND")
            
            # Show available columns for troubleshooting
            print(f"   Available columns: {list(df.columns)}")
    
    return column_mapping


def load_combined_data(file_path="scripts/claudesonnet4/version1/execution6/analysis_data/step1_combined_appointments.csv"):
    """
    Load the combined appointments dataset.
    
    Args:
        file_path (str): Path to the combined dataset file
    
    Returns:
        pd.DataFrame: Loaded dataset
    """
    
    try:
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"Combined dataset not found at: {file_path}")
        
        df = pd.read_csv(file_path, encoding='utf-8')
        print(f"✓ Loaded combined dataset: {df.shape[0]:,} rows × {df.shape[1]} columns")
        
        return df
        
    except Exception as e:
        print(f"ERROR loading combined dataset: {str(e)}")
        raise


def extract_key_columns(df, column_mapping):
    """
    Extract key columns from the dataset based on column mapping.
    
    Args:
        df (pd.DataFrame): Input dataset
        column_mapping (dict): Mapping of standard names to actual column names
    
    Returns:
        pd.DataFrame: Dataset with only key columns
    """
    
    # Check if all required columns were found
    required_columns = ['name', 'position', 'org', 'year']
    missing_required = [col for col in required_columns if col not in column_mapping]
    
    if missing_required:
        print(f"\nWARNING: Missing required columns: {missing_required}")
        print("Available columns in dataset:", list(df.columns))
        
        # Continue with available columns
        available_mapping = {k: v for k, v in column_mapping.items() if k not in missing_required}
        if not available_mapping:
            raise ValueError("No key columns could be identified!")
        column_mapping = available_mapping
    
    # Create new DataFrame with key columns
    key_df = pd.DataFrame()
    
    for standard_name, actual_column in column_mapping.items():
        key_df[standard_name] = df[actual_column].copy()
    
    print(f"\n✓ Extracted {len(column_mapping)} key columns")
    
    return key_df


def save_key_columns_data(df, output_dir="scripts/claudesonnet4/version1/execution6/analysis_data"):
    """
    Save the key columns dataset to CSV file.
    
    Args:
        df (pd.DataFrame): Key columns dataset
        output_dir (str): Directory to save the output file
    """
    
    # Create output directory if it doesn't exist
    Path(output_dir).mkdir(parents=True, exist_ok=True)
    
    # Define output file path
    output_file = os.path.join(output_dir, "step2_key_columns_data.csv")
    
    try:
        # Save to CSV
        df.to_csv(output_file, index=False, encoding='utf-8')
        print(f"✓ Saved key columns dataset to: {output_file}")
        
    except Exception as e:
        print(f"ERROR saving file: {str(e)}")
        raise


def print_extracted_info(df):
    """
    Print information about the extracted key columns dataset.
    
    Args:
        df (pd.DataFrame): Key columns dataset
    """
    
    print("\n" + "=" * 50)
    print("KEY COLUMNS DATASET SUMMARY")
    print("=" * 50)
    
    # Basic shape information
    print(f"Dataset shape: {df.shape[0]:,} rows × {df.shape[1]} columns")
    
    # Column information
    print(f"\nExtracted columns: {list(df.columns)}")
    
    # Data types
    print(f"\nData types:")
    for col, dtype in df.dtypes.items():
        print(f"  {col}: {dtype}")
    
    # Missing values analysis
    print(f"\nMissing values analysis:")
    total_rows = len(df)
    
    for col in df.columns:
        missing_count = df[col].isnull().sum()
        missing_pct = (missing_count / total_rows) * 100
        
        if missing_count > 0:
            print(f"  {col:12}: {missing_count:,} ({missing_pct:.1f}%) missing")
        else:
            print(f"  {col:12}: No missing values")
    
    # Year distribution if available
    if 'year' in df.columns and not df['year'].isnull().all():
        print(f"\nYear distribution:")
        year_counts = df['year'].value_counts().sort_index()
        for year, count in year_counts.items():
            print(f"  {year}: {count:,} records")
    
    # Reappointed distribution if available
    if 'reappointed' in df.columns and not df['reappointed'].isnull().all():
        print(f"\nReappointment distribution:")
        reappoint_counts = df['reappointed'].value_counts(dropna=False)
        for value, count in reappoint_counts.items():
            pct = (count / total_rows) * 100
            print(f"  {value}: {count:,} ({pct:.1f}%)")
    
    # Sample data
    print(f"\nSample data (first 3 rows):")
    print(df.head(3).to_string(index=False))


def main():
    """Main execution function."""
    
    print("Key Columns Extractor for NB Government Appointments")
    print("=" * 60)
    
    try:
        # Load the combined dataset
        combined_df = load_combined_data()
        
        # Identify key columns with flexible matching
        column_mapping = identify_key_columns(combined_df)
        
        if not column_mapping:
            raise ValueError("No key columns could be identified in the dataset!")
        
        # Extract key columns
        key_df = extract_key_columns(combined_df, column_mapping)
        
        # Save the key columns dataset
        save_key_columns_data(key_df)
        
        # Print summary information
        print_extracted_info(key_df)
        
        print(f"\n✓ Key columns extraction completed successfully!")
        
    except Exception as e:
        print(f"\n❌ Process failed: {str(e)}")
        sys.exit(1)


if __name__ == "__main__":
    main()