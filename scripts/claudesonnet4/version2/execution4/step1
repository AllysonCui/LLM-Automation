"""
Step 1: Combine the 12 raw datasets (2013-2024)
New Brunswick Government Appointments Analysis

This script combines all 12 CSV files containing appointment data from 2013-2024
into a single consolidated dataset for further analysis.
"""

import pandas as pd
import numpy as np
from pathlib import Path
import sys
import os

def create_output_directories():
    """Create necessary output directories if they don't exist."""
    output_dir = Path("scripts/claudesonnet4/version2/execution4/analysis_data")
    output_dir.mkdir(parents=True, exist_ok=True)
    print(f"✓ Created output directory: {output_dir}")
    return output_dir

def validate_raw_data_files():
    """Check if all required CSV files exist in the raw_data directory."""
    raw_data_dir = Path("raw_data")
    
    if not raw_data_dir.exists():
        raise FileNotFoundError(f"Raw data directory not found: {raw_data_dir}")
    
    missing_files = []
    existing_files = []
    
    for year in range(2013, 2025):  # 2013 to 2024 inclusive
        filename = f"appointments_{year}.csv"
        filepath = raw_data_dir / filename
        
        if filepath.exists():
            existing_files.append(filepath)
            print(f"✓ Found: {filename}")
        else:
            missing_files.append(filename)
            print(f"✗ Missing: {filename}")
    
    if missing_files:
        print(f"\nWarning: {len(missing_files)} files are missing:")
        for file in missing_files:
            print(f"  - {file}")
        print(f"\nProceeding with {len(existing_files)} available files...")
    
    return existing_files

def load_and_validate_csv(filepath):
    """Load a CSV file and perform basic validation."""
    try:
        df = pd.read_csv(filepath)
        
        # Extract year from filename for tracking
        year = filepath.stem.split('_')[1]
        df['source_year'] = year
        df['source_file'] = filepath.name
        
        print(f"  - Loaded {len(df)} records from {filepath.name}")
        
        # Basic validation
        if df.empty:
            print(f"    Warning: {filepath.name} is empty")
            return None
        
        # Check for essential columns (based on the sample data structure)
        expected_columns = ['name', 'position', 'org', 'reappointed']
        missing_columns = [col for col in expected_columns if col not in df.columns]
        
        if missing_columns:
            print(f"    Warning: Missing columns in {filepath.name}: {missing_columns}")
        
        # Display column info
        print(f"    Columns ({len(df.columns)}): {list(df.columns)}")
        
        return df
        
    except Exception as e:
        print(f"  ✗ Error loading {filepath.name}: {str(e)}")
        return None

def standardize_column_names(df):
    """Standardize column names across all datasets."""
    # Create a mapping for common column name variations
    column_mapping = {
        'organization': 'org',
        'Organisation': 'org',
        'Organization': 'org',
        'Name': 'name',
        'Position': 'position',
        'Reappointed': 'reappointed',
        'ReAppointed': 'reappointed',
        'Location': 'location',
        'Region': 'region',
        'Posted_Date': 'posted_date',
        'Start_Date': 'start_date',
        'End_Date': 'end_date',
        'Term_Length': 'term_length',
        'Acts': 'acts',
        'Remuneration': 'remuneration',
        'OIC': 'oic',
        'Body': 'body',
        'Link': 'link',
        'Href': 'href'
    }
    
    # Apply mapping
    df_renamed = df.rename(columns=column_mapping)
    
    # Convert column names to lowercase for consistency
    df_renamed.columns = df_renamed.columns.str.lower()
    
    return df_renamed

def combine_datasets(file_paths, output_dir):
    """Combine all CSV files into a single dataset."""
    print(f"\nCombining {len(file_paths)} datasets...")
    
    combined_data = []
    total_records = 0
    successful_files = 0
    
    for filepath in file_paths:
        print(f"\nProcessing: {filepath.name}")
        
        # Load and validate the CSV
        df = load_and_validate_csv(filepath)
        
        if df is not None:
            # Standardize column names
            df = standardize_column_names(df)
            
            # Add to combined data
            combined_data.append(df)
            total_records += len(df)
            successful_files += 1
            
            print(f"  ✓ Successfully processed {len(df)} records")
        else:
            print(f"  ✗ Failed to process {filepath.name}")
    
    if not combined_data:
        raise ValueError("No valid data files were loaded")
    
    # Combine all dataframes
    print(f"\nCombining {successful_files} datasets...")
    combined_df = pd.concat(combined_data, ignore_index=True, sort=False)
    
    print(f"✓ Combined dataset created with {len(combined_df)} total records")
    
    return combined_df

def validate_combined_data(df):
    """Perform validation on the combined dataset."""
    print("\nValidating combined dataset...")
    
    # Basic statistics
    print(f"Total records: {len(df)}")
    print(f"Total columns: {len(df.columns)}")
    print(f"Memory usage: {df.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB")
    
    # Check for duplicates
    duplicate_count = df.duplicated().sum()
    print(f"Duplicate rows: {duplicate_count}")
    
    # Year distribution
    if 'source_year' in df.columns:
        year_counts = df['source_year'].value_counts().sort_index()
        print(f"\nRecords by year:")
        for year, count in year_counts.items():
            print(f"  {year}: {count}")
    
    # Missing values analysis
    print(f"\nMissing values by column:")
    missing_counts = df.isnull().sum()
    for col, count in missing_counts.items():
        if count > 0:
            percentage = (count / len(df)) * 100
            print(f"  {col}: {count} ({percentage:.1f}%)")
    
    # Check reappointed column if it exists
    if 'reappointed' in df.columns:
        reappointed_counts = df['reappointed'].value_counts(dropna=False)
        print(f"\nReappointed value distribution:")
        for value, count in reappointed_counts.items():
            percentage = (count / len(df)) * 100
            print(f"  {value}: {count} ({percentage:.1f}%)")
    
    return True

def save_combined_dataset(df, output_dir):
    """Save the combined dataset to CSV."""
    output_file = output_dir / "step1_combined_appointments.csv"
    
    try:
        df.to_csv(output_file, index=False)
        print(f"✓ Combined dataset saved to: {output_file}")
        
        # Verify the saved file
        file_size = output_file.stat().st_size / 1024 / 1024  # MB
        print(f"  File size: {file_size:.2f} MB")
        
        return output_file
        
    except Exception as e:
        print(f"✗ Error saving combined dataset: {str(e)}")
        raise

def main():
    """Main execution function."""
    print("="*60)
    print("Step 1: Combining New Brunswick Government Appointments Data")
    print("="*60)
    
    try:
        # Create output directories
        output_dir = create_output_directories()
        
        # Validate raw data files
        file_paths = validate_raw_data_files()
        
        if not file_paths:
            raise ValueError("No valid CSV files found in raw_data directory")
        
        # Combine datasets
        combined_df = combine_datasets(file_paths, output_dir)
        
        # Validate combined data
        validate_combined_data(combined_df)
        
        # Save combined dataset
        output_file = save_combined_dataset(combined_df, output_dir)
        
        print("\n" + "="*60)
        print("STEP 1 COMPLETED SUCCESSFULLY")
        print("="*60)
        print(f"Combined dataset: {output_file}")
        print(f"Total records: {len(combined_df)}")
        print(f"Years covered: {sorted(combined_df['source_year'].unique()) if 'source_year' in combined_df.columns else 'Unknown'}")
        print("="*60)
        
    except Exception as e:
        print(f"\n✗ STEP 1 FAILED: {str(e)}")
        sys.exit(1)

if __name__ == "__main__":
    main()