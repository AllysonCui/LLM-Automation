#!/usr/bin/env python3
"""
Step 2: Extract Key Columns
Research Question: Which government branch in New Brunswick most frequently reappoints 
past appointees, and is this trend increasing or declining over the past 12 years?

This script extracts and retains the key columns needed for analysis:
"reappointed", "name", "position", "org", and "year" (derived from source_year).
"""

import pandas as pd
import numpy as np
from pathlib import Path
import sys
import os

def validate_input_file(input_file):
    """Validate that the input file exists and is readable."""
    if not input_file.exists():
        print(f"‚úó Input file not found: {input_file}")
        print("Please run Step 1 first to create the combined dataset.")
        return False
    
    try:
        # Try to read just the header to validate file structure
        sample_df = pd.read_csv(input_file, nrows=1)
        print(f"‚úì Input file validated: {input_file}")
        print(f"  Available columns: {list(sample_df.columns)}")
        return True
    except Exception as e:
        print(f"‚úó Error reading input file: {e}")
        return False

def map_column_names(df):
    """Map column names to standardized versions and handle variations."""
    column_mapping = {}
    available_cols = df.columns.tolist()
    
    print("üîç Analyzing available columns for key mappings:")
    
    # Define possible variations for each key column
    column_variations = {
        'reappointed': ['reappointed', 'reappointment', 'is_reappointed'],
        'name': ['name', 'appointee_name', 'full_name'],
        'position': ['position', 'title', 'role', 'appointment_position'],
        'org': ['org', 'organization', 'department', 'ministry', 'agency'],
        'year': ['source_year', 'year', 'appointment_year']
    }
    
    # Find matching columns
    for key_col, variations in column_variations.items():
        found = False
        for variation in variations:
            if variation in available_cols:
                column_mapping[variation] = key_col
                print(f"  ‚úì Found '{key_col}': using column '{variation}'")
                found = True
                break
        
        if not found:
            print(f"  ‚ö† Warning: No column found for '{key_col}'")
            print(f"    Looked for: {variations}")
    
    return column_mapping

def clean_reappointed_column(df):
    """Clean and standardize the reappointed column."""
    if 'reappointed' not in df.columns:
        print("‚ö† Warning: No reappointed column found for cleaning")
        return df
    
    print("üßπ Cleaning 'reappointed' column:")
    
    # Display current value distribution
    print("  Current value distribution:")
    value_counts = df['reappointed'].value_counts(dropna=False)
    for value, count in value_counts.items():
        print(f"    {repr(value)}: {count:,}")
    
    # Clean and standardize values
    original_df = df.copy()
    
    # Convert to string first to handle mixed types
    df['reappointed'] = df['reappointed'].astype(str).str.strip().str.lower()
    
    # Map various representations to boolean
    true_values = ['true', '1', 'yes', 'y', 'reappointed', 'reappointment']
    false_values = ['false', '0', 'no', 'n', 'new', 'first appointment', 'initial']
    
    df['reappointed'] = df['reappointed'].apply(lambda x: 
        True if x in true_values else 
        False if x in false_values else 
        np.nan if x in ['nan', 'none', ''] else x
    )
    
    # Check for any unmapped values
    unmapped_mask = ~df['reappointed'].isin([True, False]) & df['reappointed'].notna()
    if unmapped_mask.any():
        unmapped_values = df[unmapped_mask]['reappointed'].unique()
        print(f"  ‚ö† Warning: Unmapped values found: {unmapped_values}")
        print("    These will be treated as missing values")
        df.loc[unmapped_mask, 'reappointed'] = np.nan
    
    # Display cleaned value distribution
    print("  Cleaned value distribution:")
    cleaned_counts = df['reappointed'].value_counts(dropna=False)
    for value, count in cleaned_counts.items():
        print(f"    {repr(value)}: {count:,}")
    
    return df

def extract_key_columns():
    """Main function to extract key columns from the combined dataset."""
    print("="*60)
    print("STEP 2: EXTRACTING KEY COLUMNS")
    print("="*60)
    
    # Define paths
    input_path = Path("scripts/claudesonnet4/version2/execution4/analysis_data")
    input_file = input_path / "step1_combined_appointments.csv"
    output_file = input_path / "step2_key_columns_data.csv"
    
    # Validate input file
    if not validate_input_file(input_file):
        return False
    
    try:
        # Load the combined dataset
        print(f"\nüìÇ Loading combined dataset...")
        df = pd.read_csv(input_file)
        print(f"‚úì Loaded dataset: {len(df):,} rows, {len(df.columns)} columns")
        
        # Map column names to find key columns
        print(f"\nüóÇÔ∏è Mapping column names:")
        column_mapping = map_column_names(df)
        
        if not column_mapping:
            print("‚úó No key columns could be mapped. Cannot proceed.")
            return False
        
        # Extract and rename columns
        print(f"\n‚úÇÔ∏è Extracting key columns:")
        key_columns = []
        renamed_columns = {}
        
        for original_col, target_col in column_mapping.items():
            if original_col in df.columns:
                key_columns.append(original_col)
                renamed_columns[original_col] = target_col
                print(f"  ‚úì Extracting '{original_col}' ‚Üí '{target_col}'")
        
        # Create subset with key columns only
        df_subset = df[key_columns].copy()
        
        # Rename columns to standardized names
        df_subset = df_subset.rename(columns=renamed_columns)
        
        # Ensure we have a 'year' column (rename source_year if needed)
        if 'source_year' in df_subset.columns and 'year' not in df_subset.columns:
            df_subset = df_subset.rename(columns={'source_year': 'year'})
        
        print(f"‚úì Extracted {len(df_subset.columns)} key columns: {list(df_subset.columns)}")
        
        # Clean the reappointed column
        if 'reappointed' in df_subset.columns:
            print(f"\nüîß Processing reappointed column:")
            df_subset = clean_reappointed_column(df_subset)
        
        # Data quality analysis
        print(f"\nüìä Data Quality Analysis:")
        print(f"  Total records: {len(df_subset):,}")
        
        # Missing value analysis
        missing_analysis = df_subset.isnull().sum()
        if missing_analysis.sum() > 0:
            print(f"  Missing values by column:")
            for col, missing_count in missing_analysis.items():
                if missing_count > 0:
                    percentage = (missing_count / len(df_subset)) * 100
                    print(f"    {col}: {missing_count:,} ({percentage:.1f}%)")
        else:
            print("  ‚úì No missing values detected")
        
        # Year distribution
        if 'year' in df_subset.columns:
            print(f"  Year distribution:")
            year_counts = df_subset['year'].value_counts().sort_index()
            for year, count in year_counts.items():
                print(f"    {year}: {count:,} appointments")
        
        # Reappointment distribution
        if 'reappointed' in df_subset.columns:
            reapp_counts = df_subset['reappointed'].value_counts(dropna=False)
            print(f"  Reappointment distribution:")
            for status, count in reapp_counts.items():
                percentage = (count / len(df_subset)) * 100
                print(f"    {repr(status)}: {count:,} ({percentage:.1f}%)")
        
        # Organization distribution (top 10)
        if 'org' in df_subset.columns:
            print(f"  Top 10 organizations by appointment count:")
            org_counts = df_subset['org'].value_counts().head(10)
            for org, count in org_counts.items():
                percentage = (count / len(df_subset)) * 100
                print(f"    {org}: {count:,} ({percentage:.1f}%)")
        
        # Display sample of processed data
        print(f"\nüìã Sample of processed data (first 5 rows):")
        print(df_subset.head().to_string())
        
        # Save the key columns dataset
        print(f"\nüíæ Saving key columns dataset to: {output_file}")
        df_subset.to_csv(output_file, index=False)
        print(f"‚úì Successfully saved: {len(df_subset):,} rows, {len(df_subset.columns)} columns")
        
        # Final validation
        try:
            verification_df = pd.read_csv(output_file)
            if len(verification_df) == len(df_subset) and len(verification_df.columns) == len(df_subset.columns):
                print("‚úì File save verification successful")
            else:
                print("‚ö† Warning: Saved file dimensions don't match processed data")
        except Exception as e:
            print(f"‚ö† Warning: Could not verify saved file: {e}")
        
        print("\n" + "="*60)
        print("STEP 2 COMPLETED SUCCESSFULLY")
        print("="*60)
        print(f"Output: {output_file}")
        print(f"Extracted {len(df_subset.columns)} key columns from {len(df_subset):,} appointments")
        print("Ready for Step 3: Mark repeats identification")
        
        return True
        
    except Exception as e:
        print(f"‚úó Error processing key columns: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = extract_key_columns()
    if not success:
        print("\n‚ùå Step 2 failed. Please check the errors above and retry.")
        sys.exit(1)
    else:
        print("\n‚úÖ Step 2 completed successfully!")