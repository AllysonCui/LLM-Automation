"""
Step 2: Extract and retain key columns
New Brunswick Government Appointments Analysis

This script extracts and retains the key columns: "reappointed", "name", "position", "org", and "year"
from the combined appointments dataset created in Step 1.
"""

import pandas as pd
import numpy as np
from pathlib import Path
import sys
import os

def validate_input_file():
    """Validate that the input file from Step 1 exists."""
    input_file = Path("scripts/claudesonnet4/version2/execution4/analysis_data/step1_combined_appointments.csv")
    
    if not input_file.exists():
        raise FileNotFoundError(f"Input file not found: {input_file}")
    
    print(f"✓ Input file found: {input_file}")
    return input_file

def load_combined_data(input_file):
    """Load the combined dataset from Step 1."""
    try:
        df = pd.read_csv(input_file)
        print(f"✓ Loaded combined dataset with {len(df)} records and {len(df.columns)} columns")
        
        # Display available columns
        print(f"Available columns: {list(df.columns)}")
        
        return df
        
    except Exception as e:
        print(f"✗ Error loading combined dataset: {str(e)}")
        raise

def extract_year_column(df):
    """Extract or create the 'year' column from available data."""
    print("\nExtracting year information...")
    
    # Check if 'source_year' column exists (created in Step 1)
    if 'source_year' in df.columns:
        print("✓ Using 'source_year' column from Step 1")
        df['year'] = df['source_year']
    
    # Check for other potential year columns
    elif 'posted_date' in df.columns and not df['posted_date'].isnull().all():
        print("✓ Extracting year from 'posted_date' column")
        df['posted_date'] = pd.to_datetime(df['posted_date'], errors='coerce')
        df['year'] = df['posted_date'].dt.year.astype(str)
    
    elif 'start_date' in df.columns and not df['start_date'].isnull().all():
        print("✓ Extracting year from 'start_date' column")
        df['start_date'] = pd.to_datetime(df['start_date'], errors='coerce')
        df['year'] = df['start_date'].dt.year.astype(str)
    
    else:
        print("✗ No suitable year column found")
        raise ValueError("Unable to determine year information from available columns")
    
    # Validate year values
    unique_years = df['year'].dropna().unique()
    print(f"Year values found: {sorted(unique_years)}")
    
    return df

def standardize_reappointed_column(df):
    """Standardize the reappointed column values."""
    print("\nStandardizing 'reappointed' column...")
    
    if 'reappointed' not in df.columns:
        raise ValueError("'reappointed' column not found in dataset")
    
    # Display current distribution
    print("Current reappointed values:")
    reappointed_counts = df['reappointed'].value_counts(dropna=False)
    for value, count in reappointed_counts.items():
        percentage = (count / len(df)) * 100
        print(f"  {value}: {count} ({percentage:.1f}%)")
    
    # Standardize values to boolean
    # Handle various possible representations
    df['reappointed_original'] = df['reappointed'].copy()  # Keep original for reference
    
    # Convert to string first for consistent processing
    df['reappointed_str'] = df['reappointed'].astype(str).str.lower().str.strip()
    
    # Map various representations to boolean
    reappointed_mapping = {
        'true': True,
        'false': False,
        'yes': True,
        'no': False,
        '1': True,
        '0': False,
        'y': True,
        'n': False,
        'reappointed': True,
        'new appointment': False,
        'new': False,
        'nan': np.nan
    }
    
    df['reappointed'] = df['reappointed_str'].map(reappointed_mapping)
    
    # Handle any unmapped values
    unmapped = df[df['reappointed'].isnull() & df['reappointed_str'].notna()]
    if len(unmapped) > 0:
        print(f"Warning: {len(unmapped)} values could not be mapped:")
        unmapped_values = unmapped['reappointed_str'].value_counts()
        for value, count in unmapped_values.items():
            print(f"  '{value}': {count}")
    
    # Display standardized distribution
    print("\nStandardized reappointed values:")
    standardized_counts = df['reappointed'].value_counts(dropna=False)
    for value, count in standardized_counts.items():
        percentage = (count / len(df)) * 100
        print(f"  {value}: {count} ({percentage:.1f}%)")
    
    # Clean up temporary columns
    df.drop(['reappointed_str'], axis=1, inplace=True)
    
    return df

def extract_key_columns(df):
    """Extract the key columns: reappointed, name, position, org, and year."""
    print("\nExtracting key columns...")
    
    # Define required columns
    required_columns = ['reappointed', 'name', 'position', 'org', 'year']
    
    # Check which columns exist
    missing_columns = []
    available_columns = []
    
    for col in required_columns:
        if col in df.columns:
            available_columns.append(col)
            print(f"✓ Found: {col}")
        else:
            missing_columns.append(col)
            print(f"✗ Missing: {col}")
    
    if missing_columns:
        print(f"\nError: Missing required columns: {missing_columns}")
        print("Available columns in dataset:")
        for col in sorted(df.columns):
            print(f"  - {col}")
        raise ValueError(f"Missing required columns: {missing_columns}")
    
    # Extract key columns
    key_df = df[required_columns].copy()
    
    # Add reappointed_original for reference if it exists
    if 'reappointed_original' in df.columns:
        key_df['reappointed_original'] = df['reappointed_original']
    
    print(f"✓ Extracted {len(key_df)} records with {len(key_df.columns)} key columns")
    
    return key_df

def validate_key_columns_data(df):
    """Validate the extracted key columns data."""
    print("\nValidating key columns data...")
    
    # Basic statistics
    print(f"Total records: {len(df)}")
    print(f"Total columns: {len(df.columns)}")
    
    # Check for missing values in each key column
    print(f"\nMissing values analysis:")
    for col in ['reappointed', 'name', 'position', 'org', 'year']:
        if col in df.columns:
            missing_count = df[col].isnull().sum()
            percentage = (missing_count / len(df)) * 100
            print(f"  {col}: {missing_count} missing ({percentage:.1f}%)")
    
    # Year distribution
    if 'year' in df.columns:
        print(f"\nYear distribution:")
        year_counts = df['year'].value_counts().sort_index()
        for year, count in year_counts.items():
            print(f"  {year}: {count}")
    
    # Organization distribution (top 10)
    if 'org' in df.columns:
        print(f"\nTop 10 organizations:")
        org_counts = df['org'].value_counts().head(10)
        for org, count in org_counts.items():
            print(f"  {org}: {count}")
    
    # Position distribution (top 10)
    if 'position' in df.columns:
        print(f"\nTop 10 positions:")
        position_counts = df['position'].value_counts().head(10)
        for pos, count in position_counts.items():
            print(f"  {pos}: {count}")
    
    # Reappointed distribution
    if 'reappointed' in df.columns:
        print(f"\nReappointed distribution:")
        reappointed_counts = df['reappointed'].value_counts(dropna=False)
        for value, count in reappointed_counts.items():
            percentage = (count / len(df)) * 100
            print(f"  {value}: {count} ({percentage:.1f}%)")
    
    # Check for duplicate records
    duplicate_count = df.duplicated(subset=['name', 'position', 'org', 'year']).sum()
    print(f"\nDuplicate records (same name, position, org, year): {duplicate_count}")
    
    return True

def clean_and_standardize_data(df):
    """Clean and standardize the key columns data."""
    print("\nCleaning and standardizing data...")
    
    original_count = len(df)
    
    # Clean string columns
    string_columns = ['name', 'position', 'org', 'year']
    for col in string_columns:
        if col in df.columns:
            # Remove extra whitespace
            df[col] = df[col].astype(str).str.strip()
            
            # Replace empty strings with NaN
            df[col] = df[col].replace('', np.nan)
            df[col] = df[col].replace('nan', np.nan)
    
    # Standardize organization names (basic cleanup)
    if 'org' in df.columns:
        # Common organization name standardizations
        org_replacements = {
            'Dept.': 'Department',
            'Min.': 'Ministry',
            'Gov.': 'Government',
            'N.B.': 'New Brunswick',
            'NB': 'New Brunswick'
        }
        
        for old, new in org_replacements.items():
            df['org'] = df['org'].str.replace(old, new, regex=False)
    
    # Ensure year is string type for consistency
    if 'year' in df.columns:
        df['year'] = df['year'].astype(str)
    
    print(f"✓ Data cleaning completed. Records: {original_count} -> {len(df)}")
    
    return df

def save_key_columns_data(df, output_dir):
    """Save the key columns dataset."""
    output_file = output_dir / "step2_key_columns_data.csv"
    
    try:
        df.to_csv(output_file, index=False)
        print(f"✓ Key columns dataset saved to: {output_file}")
        
        # Verify the saved file
        file_size = output_file.stat().st_size / 1024 / 1024  # MB
        print(f"  File size: {file_size:.2f} MB")
        
        return output_file
        
    except Exception as e:
        print(f"✗ Error saving key columns dataset: {str(e)}")
        raise

def main():
    """Main execution function."""
    print("="*60)
    print("Step 2: Extract Key Columns from Combined Dataset")
    print("="*60)
    
    try:
        # Validate input file
        input_file = validate_input_file()
        
        # Load combined data
        df = load_combined_data(input_file)
        
        # Extract/create year column
        df = extract_year_column(df)
        
        # Standardize reappointed column
        df = standardize_reappointed_column(df)
        
        # Extract key columns
        key_df = extract_key_columns(df)
        
        # Clean and standardize data
        key_df = clean_and_standardize_data(key_df)
        
        # Validate extracted data
        validate_key_columns_data(key_df)
        
        # Save key columns dataset
        output_dir = Path("scripts/claudesonnet4/version2/execution4/analysis_data")
        output_file = save_key_columns_data(key_df, output_dir)
        
        print("\n" + "="*60)
        print("STEP 2 COMPLETED SUCCESSFULLY")
        print("="*60)
        print(f"Key columns dataset: {output_file}")
        print(f"Total records: {len(key_df)}")
        print(f"Key columns: {list(key_df.columns)}")
        print(f"Years covered: {sorted(key_df['year'].unique())}")
        print("="*60)
        
    except Exception as e:
        print(f"\n✗ STEP 2 FAILED: {str(e)}")
        sys.exit(1)

if __name__ == "__main__":
    main()