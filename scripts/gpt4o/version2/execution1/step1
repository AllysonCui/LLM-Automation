# File: scripts/gpt4o/version2/execution1/step1_combine_datasets.py

import pandas as pd
from pathlib import Path

# Define input and output paths
RAW_DATA_DIR = Path("raw_data")
OUTPUT_DIR = Path("scripts/gpt4o/version2/execution1/analysis_data")
OUTPUT_FILE = OUTPUT_DIR / "step1_combined_appointments.csv"

def combine_datasets():
    try:
        # Ensure output directory exists
        OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
        
        all_dfs = []
        missing_files = []
        print("Starting dataset combination...")

        for year in range(2013, 2025):
            file_path = RAW_DATA_DIR / f"appointments_{year}.csv"
            if not file_path.exists():
                print(f"Warning: File not found: {file_path}")
                missing_files.append(str(file_path))
                continue

            try:
                df = pd.read_csv(file_path)
                df["year"] = year  # Tag each row with the year of appointment
                all_dfs.append(df)
                print(f"Loaded {file_path} with {len(df)} records.")
            except Exception as e:
                print(f"Error reading {file_path}: {e}")
        
        if not all_dfs:
            raise ValueError("No data files were successfully loaded.")

        combined_df = pd.concat(all_dfs, ignore_index=True)
        print(f"Combined dataset has {len(combined_df)} total records from {len(all_dfs)} files.")

        # Save to CSV
        combined_df.to_csv(OUTPUT_FILE, index=False)
        print(f"Combined dataset saved to {OUTPUT_FILE}")

        # Optionally report missing files
        if missing_files:
            print(f"\nMissing files ({len(missing_files)}):")
            for f in missing_files:
                print(f" - {f}")

    except Exception as e:
        print(f"Unexpected error during dataset combination: {e}")

if __name__ == "__main__":
    combine_datasets()
