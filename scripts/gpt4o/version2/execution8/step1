import pandas as pd
from pathlib import Path

# Define paths
raw_data_dir = Path("raw_data")
output_dir = Path("scripts/gpt4o/version2/execution8/analysis_data")
output_file = output_dir / "step1_combined_appointments.csv"

# Ensure output directory exists
output_dir.mkdir(parents=True, exist_ok=True)

# Prepare to collect dataframes
all_dfs = []
missing_files = []

print("Starting Step 1: Combine all yearly appointment data files...")

# Iterate through the years and read each CSV
for year in range(2013, 2025):
    file_path = raw_data_dir / f"appointments_{year}.csv"
    if file_path.exists():
        try:
            df = pd.read_csv(file_path)
            df['source_year'] = year  # Track origin year for reference
            all_dfs.append(df)
            print(f"Loaded: {file_path.name} with {len(df)} rows.")
        except Exception as e:
            print(f"Error reading {file_path.name}: {e}")
    else:
        missing_files.append(file_path.name)
        print(f"Missing file: {file_path.name}")

# Combine all loaded dataframes
if all_dfs:
    combined_df = pd.concat(all_dfs, ignore_index=True)
    combined_df.to_csv(output_file, index=False)
    print(f"\nCombined {len(all_dfs)} files into {output_file.name} with {len(combined_df)} total rows.")
else:
    print("No files were loaded. Check that raw data files exist in the expected directory.")

# Report any missing files
if missing_files:
    print(f"\nWarning: The following files were not found and skipped:")
    for fname in missing_files:
        print(f" - {fname}")
